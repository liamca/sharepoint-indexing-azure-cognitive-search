{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Content from all Folders in SharePoint Site to an Azure AI Search Index\n",
    "Includes Vectorizaztion of Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import requests\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents import SearchClient\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt \n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "from langchain.text_splitter import TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define the target directory (change yours)\n",
    "target_directory = (\n",
    "    r\"C:\\temp\\docker\\sharepoint-indexer\\sharepoint-indexing-azure-cognitive-search\"\n",
    ")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the service endpoint and API key from the environment\n",
    "# Create an SDK client\n",
    "endpoint = os.environ[\"SEARCH_SERVICE_ENDPOINT\"]\n",
    "search_client = SearchClient(\n",
    "    endpoint=endpoint,\n",
    "    index_name=os.environ[\"SEARCH_INDEX_NAME\"],\n",
    "    credential=AzureKeyCredential(os.environ[\"SEARCH_ADMIN_API_KEY\"]),\n",
    ")\n",
    "\n",
    "admin_client = SearchIndexClient(\n",
    "    endpoint=endpoint,\n",
    "    index_name=os.environ[\"SEARCH_INDEX_NAME\"],\n",
    "    credential=AzureKeyCredential(os.environ[\"SEARCH_ADMIN_API_KEY\"]),\n",
    ")\n",
    "\n",
    "openai.api_key = os.environ[\"OPEN_API_KEY\"]\n",
    "openai.api_base = os.environ[\"OPEN_API_BASE\"]\n",
    "openai.api_type = \"azure\"  \n",
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "model = os.environ[\"OPEN_API_MODEL\"]\n",
    "\n",
    "client = AzureOpenAI(\n",
    "        api_version=openai.api_version,\n",
    "        azure_endpoint=openai.api_base,\n",
    "        api_key=openai.api_key\n",
    "    )\n",
    "\n",
    "# This is in characters and there is an avg of 4 chars / token\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1024*4,\n",
    "    chunk_overlap  = 102*4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `client_extractor` client <a id='init-client'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gbb_ai.sharepoint_data_extractor import SharePointDataExtractor\n",
    "\n",
    "# Instantiate the SharePointDataExtractor client\n",
    "# The client handles the complexities of interacting with SharePoint's REST API, providing an easy-to-use interface for data extraction.\n",
    "client_scrapping = SharePointDataExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Note**\n",
    "> The `get_site_id` and `get_drive_id` methods are optional. They are automatically called by the `retrieve_sharepoint_files_content` function. However, they are available for use if further analysis is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the .env file\n",
    "client_scrapping.load_environment_variables_from_env_file()\n",
    "\n",
    "# Authenticate with Microsoft Graph API\n",
    "client_scrapping.msgraph_auth()\n",
    "\n",
    "# Get the Site ID for the specified SharePoint site\n",
    "site_id = client_scrapping.get_site_id(\n",
    "    site_domain=os.environ[\"SITE_DOMAIN\"], site_name=os.environ[\"SITE_NAME\"]\n",
    ")\n",
    "\n",
    "# Get the Drive ID associated with the Site ID\n",
    "drive_id = client_scrapping.get_drive_id(site_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folders(url, folder_list = ['/']):\n",
    "    headers = {'Authorization': 'Bearer ' + client_scrapping.access_token}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    items = response.json()\n",
    "\n",
    "    # Check if the 'value' key is in the response\n",
    "    if 'value' not in items:\n",
    "        return folder_list\n",
    "\n",
    "    for item in items['value']:\n",
    "        if 'folder' in item:\n",
    "            # print(item['name'] + '/')\n",
    "            # If the item is a folder, get its subfolders\n",
    "            subfolder_url = url + '/' + item['name'] + '/children'\n",
    "            folder_val = subfolder_url[subfolder_url.index('/drive/root')+11:].replace('/children','') + '/'\n",
    "            print (folder_val)\n",
    "            folder_list.append(folder_val)\n",
    "            get_folders(subfolder_url, folder_list)\n",
    "    return folder_list\n",
    "\n",
    "# Split up a list into chunks - this is used to ensure a limited number of items sent to Azure AI Search\n",
    "def divide_chunks(l, n):  \n",
    "    # looping till length l  \n",
    "    for i in range(0, len(l), n):   \n",
    "        yield l[i:i + n]  \n",
    "\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def generate_embeddings(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    return json.loads(response.model_dump_json())[\"data\"][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_embeddings('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the access token to get the folders \n",
    "print ('Getting all folders in SharePoint site...')\n",
    "root_url = f'https://graph.microsoft.com/v1.0/sites/{site_id}/drive/root/children'  \n",
    "folder_list = get_folders(root_url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and process files from a set of folders within a SharePoint site.\n",
    "\n",
    "n = 100  # max batch size (number of docs) to upload at a time\n",
    "total_docs_uploaded = 0\n",
    "\n",
    "for folder in folder_list:\n",
    "    print (f\"Processing folder {folder}...\")\n",
    "\n",
    "    if folder == '/':\n",
    "        selected_files_content = client_scrapping.retrieve_sharepoint_files_content(\n",
    "            site_domain=os.environ[\"SITE_DOMAIN\"],\n",
    "            site_name=os.environ[\"SITE_NAME\"],\n",
    "            file_formats=[\"docx\", \"pdf\"],\n",
    "        )\n",
    "    else:\n",
    "        selected_files_content = client_scrapping.retrieve_sharepoint_files_content(\n",
    "            site_domain=os.environ[\"SITE_DOMAIN\"],\n",
    "            site_name=os.environ[\"SITE_NAME\"],\n",
    "            folder_path=folder,\n",
    "            file_formats=[\"docx\", \"pdf\"],\n",
    "        )\n",
    "\n",
    "    if selected_files_content == None:\n",
    "        print (\"No documents found in this folder\")\n",
    "    else:\n",
    "        chunked_content_docs = []\n",
    "        sfc_counter = 0\n",
    "        for sfc_counter in range(len(selected_files_content)):\n",
    "            # print (json_data_base['content'])\n",
    "            chunked_content = text_splitter.split_text(selected_files_content[sfc_counter]['content'])\n",
    "            chunk_counter = 0\n",
    "            for cc in chunked_content:\n",
    "                json_data = copy.deepcopy(selected_files_content[sfc_counter]) \n",
    "                json_data['content'] = chunked_content[chunk_counter]\n",
    "                json_data['contentVector'] = generate_embeddings(json_data['content'])\n",
    "                json_data['doc_id'] = json_data['id']\n",
    "                json_data['id'] = json_data['id'] + \"-\" + str(chunk_counter)\n",
    "                json_data['chunk_id'] = chunk_counter\n",
    "                chunk_counter+=1\n",
    "                chunked_content_docs.append(json_data)\n",
    "            sfc_counter+=1\n",
    "            \n",
    "        total_docs = len(chunked_content_docs)\n",
    "        total_docs_uploaded += total_docs\n",
    "        print (f\"Total Documents to Upload: {total_docs}\")\n",
    "\n",
    "        for documents_chunk in divide_chunks(chunked_content_docs, n):  \n",
    "            # Multiple Documents Upload\n",
    "            try:\n",
    "                # 'search_client.upload_documents' can ingest multiple documents at once\n",
    "                # 'selected_files_content' is a list of documents\n",
    "                print (f\"Uploading batch of {len(documents_chunk)} documents...\")\n",
    "                result = search_client.upload_documents(documents=documents_chunk)\n",
    "                # Print the result for each document\n",
    "                for res in result:\n",
    "                    print(\"Upload of new document succeeded: {}\".format(res.succeeded))\n",
    "            except Exception as ex:\n",
    "                print(\"Error in multiple documents upload: \", ex)\n",
    "    # print (selected_files_content)\n",
    "\n",
    "print (f\"Upload of {total_docs_uploaded} documents complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
