{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Content from all Folders in SharePoint Site to an Azure AI Search Index\n",
    "Includes Vectorizaztion of Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to /Users/marcjimz/Documents/Development/sharepoint-indexing-azure-cognitive-search\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import requests\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents import SearchClient\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt \n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "from langchain.text_splitter import TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define the target directory (change yours)\n",
    "target_directory = (\n",
    "    os.getcwd()\n",
    ")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the service endpoint and API key from the environment\n",
    "# Create an SDK client\n",
    "endpoint = os.environ[\"SEARCH_SERVICE_ENDPOINT\"]\n",
    "search_client = SearchClient(\n",
    "    endpoint=endpoint,\n",
    "    index_name=os.environ[\"SEARCH_INDEX_NAME\"],\n",
    "    credential=AzureKeyCredential(os.environ[\"SEARCH_ADMIN_API_KEY\"]),\n",
    ")\n",
    "\n",
    "admin_client = SearchIndexClient(\n",
    "    endpoint=endpoint,\n",
    "    index_name=os.environ[\"SEARCH_INDEX_NAME\"],\n",
    "    credential=AzureKeyCredential(os.environ[\"SEARCH_ADMIN_API_KEY\"]),\n",
    ")\n",
    "\n",
    "openai.api_key = os.environ[\"OPEN_API_KEY\"]\n",
    "openai.api_base = os.environ[\"OPEN_API_BASE\"]\n",
    "openai.api_type = \"azure\"  \n",
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "model = os.environ[\"OPEN_API_MODEL\"]\n",
    "\n",
    "client = AzureOpenAI(\n",
    "        api_version=openai.api_version,\n",
    "        azure_endpoint=openai.api_base,\n",
    "        api_key=openai.api_key\n",
    "    )\n",
    "\n",
    "# This is in characters and there is an avg of 4 chars / token\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1024*4,\n",
    "    chunk_overlap  = 102*4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `client_extractor` client <a id='init-client'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gbb_ai.sharepoint_data_extractor import SharePointDataExtractor\n",
    "\n",
    "# Instantiate the SharePointDataExtractor client\n",
    "# The client handles the complexities of interacting with SharePoint's REST API, providing an easy-to-use interface for data extraction.\n",
    "client_scrapping = SharePointDataExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Note**\n",
    "> The `get_site_id` and `get_drive_id` methods are optional. They are automatically called by the `retrieve_sharepoint_files_content` function. However, they are available for use if further analysis is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 22:11:03,595 - micro - MainProcess - INFO     Successfully loaded environment variables: TENANT_ID, CLIENT_ID, CLIENT_SECRET (sharepoint_data_extractor.py:load_environment_variables_from_env_file:86)\n",
      "2024-09-03 22:11:03,981 - micro - MainProcess - INFO     New access token retrieved. (sharepoint_data_extractor.py:msgraph_auth:118)\n",
      "2024-09-03 22:11:03,982 - micro - MainProcess - INFO     Getting the Site ID... (sharepoint_data_extractor.py:get_site_id:187)\n",
      "2024-09-03 22:11:04,318 - micro - MainProcess - INFO     Site ID retrieved: 30z44s.sharepoint.com,4303930e-50c4-467a-ac6e-2128d74f3554,6828085b-3888-432a-baa6-225475f35b6b (sharepoint_data_extractor.py:get_site_id:191)\n",
      "2024-09-03 22:11:04,738 - micro - MainProcess - INFO     Successfully retrieved drive ID: b!DpMDQ8RQekasbiEo1081VFsIKGiIOCpDuqYiVHXzW2vabjOBwPZiQ4_E_CuTBjAI (sharepoint_data_extractor.py:get_drive_id:208)\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from the .env file\n",
    "client_scrapping.load_environment_variables_from_env_file()\n",
    "\n",
    "# Authenticate with Microsoft Graph API\n",
    "client_scrapping.msgraph_auth()\n",
    "\n",
    "# Get the Site ID for the specified SharePoint site\n",
    "site_id = client_scrapping.get_site_id(\n",
    "    site_domain=os.environ[\"SITE_DOMAIN\"], site_name=os.environ[\"SITE_NAME\"]\n",
    ")\n",
    "\n",
    "# Get the Drive ID associated with the Site ID\n",
    "drive_id = client_scrapping.get_drive_id(site_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folders(url, folder_list = ['/']):\n",
    "    headers = {'Authorization': 'Bearer ' + client_scrapping.access_token}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    items = response.json()\n",
    "\n",
    "    # Check if the 'value' key is in the response\n",
    "    if 'value' not in items:\n",
    "        return folder_list\n",
    "\n",
    "    for item in items['value']:\n",
    "        if 'folder' in item:\n",
    "            # print(item['name'] + '/')\n",
    "            # If the item is a folder, get its subfolders\n",
    "            subfolder_url = url + '/' + item['name'] + '/children'\n",
    "            folder_val = subfolder_url[subfolder_url.index('/drive/root')+11:].replace('/children','') + '/'\n",
    "            print (folder_val)\n",
    "            folder_list.append(folder_val)\n",
    "            get_folders(subfolder_url, folder_list)\n",
    "    return folder_list\n",
    "\n",
    "# Split up a list into chunks - this is used to ensure a limited number of items sent to Azure AI Search\n",
    "def divide_chunks(l, n):  \n",
    "    # looping till length l  \n",
    "    for i in range(0, len(l), n):   \n",
    "        yield l[i:i + n]  \n",
    "\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def generate_embeddings(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    return json.loads(response.model_dump_json())[\"data\"][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_embeddings('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting all folders in SharePoint site...\n"
     ]
    }
   ],
   "source": [
    "# Use the access token to get the folders \n",
    "print ('Getting all folders in SharePoint site...')\n",
    "root_url = f'https://graph.microsoft.com/v1.0/sites/{site_id}/drive/root/children'  \n",
    "folder_list = get_folders(root_url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://graph.microsoft.com/v1.0/sites/30z44s.sharepoint.com,4303930e-50c4-467a-ac6e-2128d74f3554,6828085b-3888-432a-baa6-225475f35b6b/drive/root/children'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 22:11:05,217 - micro - MainProcess - INFO     Getting the Site ID... (sharepoint_data_extractor.py:get_site_id:187)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder /...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 22:11:05,527 - micro - MainProcess - INFO     Site ID retrieved: 30z44s.sharepoint.com,4303930e-50c4-467a-ac6e-2128d74f3554,6828085b-3888-432a-baa6-225475f35b6b (sharepoint_data_extractor.py:get_site_id:191)\n",
      "2024-09-03 22:11:05,922 - micro - MainProcess - INFO     Successfully retrieved drive ID: b!DpMDQ8RQekasbiEo1081VFsIKGiIOCpDuqYiVHXzW2vabjOBwPZiQ4_E_CuTBjAI (sharepoint_data_extractor.py:get_drive_id:208)\n",
      "2024-09-03 22:11:05,923 - micro - MainProcess - INFO     Making request to Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:247)\n",
      "2024-09-03 22:11:06,136 - micro - MainProcess - INFO     Received response from Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:250)\n",
      "2024-09-03 22:11:06,137 - micro - MainProcess - ERROR    No files found in the site's drive (sharepoint_data_extractor.py:retrieve_sharepoint_files_content:536)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No documents found in this folder\n",
      "Upload of 0 documents complete.\n"
     ]
    }
   ],
   "source": [
    "# Download and process files from a set of folders within a SharePoint site.\n",
    "\n",
    "n = 100  # max batch size (number of docs) to upload at a time\n",
    "total_docs_uploaded = 0\n",
    "\n",
    "for folder in folder_list:\n",
    "    print (f\"Processing folder {folder}...\")\n",
    "\n",
    "    if folder == '/':\n",
    "        selected_files_content = client_scrapping.retrieve_sharepoint_files_content(\n",
    "            site_domain=os.environ[\"SITE_DOMAIN\"],\n",
    "            site_name=os.environ[\"SITE_NAME\"],\n",
    "            file_formats=[\"docx\", \"pdf\"],\n",
    "        )\n",
    "    else:\n",
    "        selected_files_content = client_scrapping.retrieve_sharepoint_files_content(\n",
    "            site_domain=os.environ[\"SITE_DOMAIN\"],\n",
    "            site_name=os.environ[\"SITE_NAME\"],\n",
    "            folder_path=folder,\n",
    "            file_formats=[\"docx\", \"pdf\"],\n",
    "        )\n",
    "\n",
    "    if selected_files_content == None:\n",
    "        print (\"No documents found in this folder\")\n",
    "    else:\n",
    "        chunked_content_docs = []\n",
    "        sfc_counter = 0\n",
    "        for sfc_counter in range(len(selected_files_content)):\n",
    "            # print (json_data_base['content'])\n",
    "            chunked_content = text_splitter.split_text(selected_files_content[sfc_counter]['content'])\n",
    "            chunk_counter = 0\n",
    "            for cc in chunked_content:\n",
    "                json_data = copy.deepcopy(selected_files_content[sfc_counter]) \n",
    "                json_data['content'] = chunked_content[chunk_counter]\n",
    "                json_data['contentVector'] = generate_embeddings(json_data['content'])\n",
    "                json_data['doc_id'] = json_data['id']\n",
    "                json_data['id'] = json_data['id'] + \"-\" + str(chunk_counter)\n",
    "                json_data['chunk_id'] = chunk_counter\n",
    "                chunk_counter+=1\n",
    "                chunked_content_docs.append(json_data)\n",
    "            sfc_counter+=1\n",
    "            \n",
    "        total_docs = len(chunked_content_docs)\n",
    "        total_docs_uploaded += total_docs\n",
    "        print (f\"Total Documents to Upload: {total_docs}\")\n",
    "\n",
    "        for documents_chunk in divide_chunks(chunked_content_docs, n):  \n",
    "            # Multiple Documents Upload\n",
    "            try:\n",
    "                # 'search_client.upload_documents' can ingest multiple documents at once\n",
    "                # 'selected_files_content' is a list of documents\n",
    "                print (f\"Uploading batch of {len(documents_chunk)} documents...\")\n",
    "                result = search_client.upload_documents(documents=documents_chunk)\n",
    "                # Print the result for each document\n",
    "                for res in result:\n",
    "                    print(\"Upload of new document succeeded: {}\".format(res.succeeded))\n",
    "            except Exception as ex:\n",
    "                print(\"Error in multiple documents upload: \", ex)\n",
    "    # print (selected_files_content)\n",
    "\n",
    "print (f\"Upload of {total_docs_uploaded} documents complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_from_webparts(webparts: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Extracts and concatenates text from a list of webparts, stripping all HTML tags.\n",
    "    \n",
    "    :param webparts: List of webparts that contain HTML content.\n",
    "    :return: A concatenated string of text with HTML tags removed.\n",
    "    \"\"\"\n",
    "    text_content = \"\"\n",
    "    for webpart in webparts:\n",
    "        if \"innerHtml\" in webpart:\n",
    "            html_content = webpart.get(\"innerHtml\", \"\")\n",
    "            # Use BeautifulSoup to strip HTML tags\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            text_content += soup.get_text(separator=' ', strip=True) + \" \"\n",
    "    \n",
    "    return text_content.strip()\n",
    "\n",
    "def extract_text_from_canvas_layout(canvas_layout: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Extracts all text from the canvasLayout by iterating through horizontalSections, columns, and webparts.\n",
    "    \n",
    "    :param canvas_layout: The canvasLayout object from a SharePoint page.\n",
    "    :return: A concatenated string of all text extracted from the layout, with HTML tags removed.\n",
    "    \"\"\"\n",
    "    text_content = \"\"\n",
    "\n",
    "    # Iterate over horizontal sections\n",
    "    horizontal_sections = canvas_layout.get(\"horizontalSections\", [])\n",
    "    for section in horizontal_sections:\n",
    "        section_id = section.get(\"id\")\n",
    "        # print(f\"Processing section {section_id} with layout {section.get('layout')}...\")\n",
    "\n",
    "        # Iterate over columns in each horizontal section\n",
    "        columns = section.get(\"columns\", [])\n",
    "        for column in columns:\n",
    "            column_id = column.get(\"id\")\n",
    "            # print(f\"Processing column {column_id} in section {section_id}...\")\n",
    "\n",
    "            # Extract text from each column's webParts\n",
    "            webparts = column.get(\"webparts\", [])\n",
    "            text_content += extract_text_from_webparts(webparts) + \" \"\n",
    "\n",
    "    return text_content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs_uploaded = 0\n",
    "\n",
    "# Retrieve and process site pages\n",
    "processed_pages_content = client_scrapping.retrieve_and_process_site_pages(site_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 4 pages from the site...\n",
      "[Page 1/4] Processing Page ID 41a9a211-512f-4841-a66b-744e685ac95c...\n",
      "[Page 1/4] Extracted text length: 1957 characters.\n",
      "[Page 1/4] Split into 1 chunks.\n",
      "[Page 1/4][Chunk 1/1] Processing chunk of size 1957 characters.\n",
      "[Page 2/4] Processing Page ID 9883ffad-80a7-4038-899a-542aa6f77e39...\n",
      "[Page 2/4] Page ID 9883ffad-80a7-4038-899a-542aa6f77e39: No text extracted from canvas layout.\n",
      "[Page 3/4] Processing Page ID 6a8b2d03-d26c-44fc-972a-c52cb449aeb1...\n",
      "[Page 3/4] Extracted text length: 4050 characters.\n",
      "[Page 3/4] Split into 1 chunks.\n",
      "[Page 3/4][Chunk 1/1] Processing chunk of size 4050 characters.\n",
      "[Page 4/4] Processing Page ID 2955533b-8c34-44db-b23b-e90fda2f80e5...\n",
      "[Page 4/4] Page ID 2955533b-8c34-44db-b23b-e90fda2f80e5: No text extracted from canvas layout.\n",
      "Total Documents ready for upload: 2\n",
      "Uploading batch of 2 documents...\n",
      "Error during multiple documents upload: 'IndexingResult' object is not subscriptable\n",
      "Total Documents Uploaded: 2.\n"
     ]
    }
   ],
   "source": [
    "total_docs_uploaded = 0\n",
    "\n",
    "# Retrieve and process site pages\n",
    "processed_pages_content = client_scrapping.retrieve_and_process_site_pages(site_id)\n",
    "\n",
    "if not processed_pages_content:\n",
    "    print(\"No pages found in the site.\")\n",
    "else:\n",
    "    chunked_content_docs = []\n",
    "    \n",
    "    print(f\"Processing {len(processed_pages_content)} pages from the site...\")\n",
    "\n",
    "    # Iterate through the processed pages content\n",
    "    for page_num, page in enumerate(processed_pages_content, start=1):\n",
    "        page_id = page.get(\"page_id\")\n",
    "        canvas_layout = page.get(\"content\", {}).get(\"canvasLayout\", {})\n",
    "\n",
    "        if not canvas_layout:\n",
    "            print(f\"[Page {page_num}/{len(processed_pages_content)}] Page ID {page_id}: No content found.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"[Page {page_num}/{len(processed_pages_content)}] Processing Page ID {page_id}...\")\n",
    "\n",
    "        # Extract all text from the canvasLayout, removing HTML\n",
    "        page_text_content = extract_text_from_canvas_layout(canvas_layout)\n",
    "\n",
    "        if not page_text_content:\n",
    "            print(f\"[Page {page_num}/{len(processed_pages_content)}] Page ID {page_id}: No text extracted from canvas layout.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"[Page {page_num}/{len(processed_pages_content)}] Extracted text length: {len(page_text_content)} characters.\")\n",
    "        \n",
    "        # Now chunk the text content - you can bring your own text splitter here and chunk accordingly!\n",
    "        chunked_content = text_splitter.split_text(page_text_content)\n",
    "        print(f\"[Page {page_num}/{len(processed_pages_content)}] Split into {len(chunked_content)} chunks.\")\n",
    "        \n",
    "        chunk_counter = 0\n",
    "\n",
    "        # Iterate through the chunks and create the chunked content docs\n",
    "        for chunk in chunked_content:\n",
    "            print(f\"[Page {page_num}/{len(processed_pages_content)}][Chunk {chunk_counter + 1}/{len(chunked_content)}] Processing chunk of size {len(chunk)} characters.\")\n",
    "\n",
    "            json_data = {\n",
    "                \"id\": page_id + \"-\" + str(chunk_counter),  # Create a unique chunk ID\n",
    "                \"content\": chunk,  # Chunked content\n",
    "                \"contentVector\": generate_embeddings(chunk),  # Embeddings for the chunk\n",
    "                \"doc_id\": page_id,  # Original page ID\n",
    "                \"chunk_id\": chunk_counter  # Chunk counter\n",
    "            }\n",
    "            chunked_content_docs.append(json_data)\n",
    "            chunk_counter += 1\n",
    "\n",
    "    # Calculate total documents to upload\n",
    "    total_docs = len(chunked_content_docs)\n",
    "    total_docs_uploaded += total_docs\n",
    "    print(f\"Total Documents ready for upload: {total_docs}\")\n",
    "\n",
    "    # Upload the documents in chunks\n",
    "    for documents_chunk in divide_chunks(chunked_content_docs, n):\n",
    "        try:\n",
    "            print(f\"Uploading batch of {len(documents_chunk)} documents...\")\n",
    "            result = search_client.upload_documents(documents=documents_chunk)\n",
    "            # Print the result for each document\n",
    "            for res in result:\n",
    "                print(f\"Upload of document {res['key']} succeeded: {res['succeeded']}\")\n",
    "        except Exception as ex:\n",
    "            print(f\"Error during multiple documents upload: {ex}\")\n",
    "\n",
    "print(f\"Total Documents Uploaded: {total_docs_uploaded}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
